{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wavelets as wl\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a dictionary with mice's experiment information and another with the spiking activity in h5py format  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of mice, classified by condition\n",
    "IDs_WT = ['SERT1597'] #, 'SERT1659', 'SERT1678', 'SERT1908', 'SERT1984', 'SERT1985', 'SERT2014']\n",
    "#IDs_KO = ['SERT1668'] #, 'SERT1665', 'SERT2013', 'SERT2018', 'SERT2024']\n",
    "\n",
    "# Dictionaries to collect info about the experiment and spiking activity\n",
    "all_info_WT = {}\n",
    "all_mice_WT = {}\n",
    "\n",
    "\n",
    "print('Processing wild-types')\n",
    "for ID in IDs_WT:\n",
    "    npys_dir = '/home/maspe/filer/SERT/' + ID + '/npys/'\n",
    "    spikes_dir = '/home/maspe/filer/SERT/' + ID + '/spikes/results/'\n",
    "    \n",
    "    print('Loading ' + ID)\n",
    "    with open(npys_dir + ID + '.info', 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "    \n",
    "    # Put the info of this mouse into de info dictionary\n",
    "    all_info_WT[ID] = info\n",
    "    channels = info['channels_list']\n",
    "    \n",
    "    fit = []\n",
    "    for channel in channels:\n",
    "        path = spikes_dir + channel + '.result.hdf5'\n",
    "    \n",
    "        # Get the spiking activity from the h5py file\n",
    "        fit.append(h5py.File(path, 'r'))\n",
    "        \n",
    "    print('Loaded ' + spikes_dir)\n",
    "     \n",
    "    # Put the spiking activity of this mouse into the spiking dictionary\n",
    "    all_mice_WT[ID] = fit\n",
    "\n",
    "    \n",
    "# Same for KO mice\n",
    "#print('Processing knock-out')\n",
    "all_info_KO = {}\n",
    "all_mice_KO = {}\n",
    "for ID in IDs_KO:\n",
    "    npys_dir = '/home/maspe/filer/SERT/' + ID + '/npys/'\n",
    "    spikes_dir = '/home/maspe/filer/SERT/' + ID + '/spikes/results/'\n",
    "    \n",
    "    print('Loading ' + ID)\n",
    "    with open(npys_dir + ID + '.info', 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "\n",
    "    all_info_KO[ID] = info\n",
    "    channels = info['channels_list']\n",
    "    \n",
    "    fit = []\n",
    "    for channel in channels:\n",
    "        path = spikes_dir + channel + '.result.hdf5'\n",
    "    \n",
    "        fit.append(h5py.File(path, 'r'))\n",
    "        \n",
    "    print('Loaded ' + spikes_dir)\n",
    "        \n",
    "    all_mice_KO[ID] = fit\n",
    "\n",
    "    \n",
    "# all_mice_WT['SERT1597'][0]['spiketimes']['temp_0'][()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one dictionary with the ID of each channel and another with the spike times for all units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_units_WT = {}\n",
    "all_channels_WT = {}\n",
    "for mouse in all_mice_WT.keys():\n",
    "    n_channels = 32\n",
    "    units = []\n",
    "    channels_id = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for channel in range(n_channels):\n",
    "        for unit in all_mice_WT[mouse][channel]['spiketimes'].keys():\n",
    "            units.append(all_mice_WT[mouse][channel]['spiketimes'][unit][()]) # Final \"[()]\" is to import values from h5py \n",
    "      \n",
    "            channels_id.append(all_info_WT[mouse]['channels_locs'][iteration])\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "            \n",
    "    all_units_WT[mouse] = units\n",
    "    all_channels_WT[mouse] = channels_id\n",
    "        \n",
    "# print('n units = %i' %len(all_units))\n",
    "\n",
    "\n",
    "all_units_KO = {}\n",
    "all_channels_KO = {}\n",
    "for mouse in all_mice_KO.keys():\n",
    "    n_channels = 32\n",
    "    units = []\n",
    "    channels_id = []\n",
    "    \n",
    "    iteration = 0\n",
    "    for channel in range(n_channels):\n",
    "        for unit in all_mice_KO[mouse][channel]['spiketimes'].keys():\n",
    "            units.append(all_mice_KO[mouse][channel]['spiketimes'][unit][()]) # Final \"[()]\" is to import values from h5py \n",
    "      \n",
    "            channels_id.append(all_info_KO[mouse]['channels_locs'][iteration])\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "            \n",
    "    all_units_KO[mouse] = units\n",
    "    all_channels_KO[mouse] = channels_id\n",
    "        \n",
    "# print('n units = %i' %len(all_units))\n",
    "# all_units_WT['SERT1597']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the windows of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently using a time windows of 2 minutes that finishes 15 seconds before the beginning of the OF, for the HC condition; and a time windows that begins 15 seconds after the beginning of the OF, and extends to minute 10 (total length: 9.75 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS CAN BE MOVED TO THE CREATION OF INFO FILES!!! ###\n",
    "### Extract a windows of n_secs length at the beginning and end of the task\n",
    "n_min = 2\n",
    "exclude_sec = 15\n",
    "sample_rate = 30000\n",
    "#window = int(sampleRate * secs)\n",
    "n_points = 30000 * 60 * 10 # Length of the OF\n",
    "window = sample_rate * n_min * 60 # 2 min windows\n",
    "exclude_window = sample_rate * exclude_sec # 15 sec windows to exclude at the HC-OF transition\n",
    "\n",
    "all_epochs_WT = {}\n",
    "all_perispikes_WT = {}\n",
    "for mouse in all_mice_WT.keys():\n",
    "    stopHC = np.int(all_info_WT[mouse]['startOF']) - exclude_window\n",
    "    startHC = stopHC - window\n",
    "    \n",
    "    startOF = np.int(all_info_WT[mouse]['startOF']) + exclude_window\n",
    "    stopOF = startOF + n_points - exclude_window\n",
    "    \n",
    "    baseline = np.arange(startHC, stopHC, 1)\n",
    "    task = np.arange(startOF, stopOF, 1)\n",
    "    \n",
    "    complete_window = np.concatenate([baseline, task])\n",
    "    all_epochs_WT[mouse] = complete_window\n",
    "    \n",
    "\n",
    "all_epochs_KO = {}\n",
    "all_perispikes_KO = {}\n",
    "for mouse in all_mice_KO.keys():\n",
    "    stopHC = np.int(all_info_KO[mouse]['startOF']) - exclude_window\n",
    "    startHC = stopHC - window\n",
    "    \n",
    "    startOF = np.int(all_info_KO[mouse]['startOF']) + exclude_window\n",
    "    stopOF = startOF + n_points - exclude_window\n",
    "    \n",
    "    baseline = np.arange(startHC, stopHC, 1)\n",
    "    task = np.arange(startOF, stopOF, 1)\n",
    "    \n",
    "    complete_window = np.concatenate([baseline, task])\n",
    "    all_epochs_KO[mouse] = complete_window\n",
    "\n",
    "\n",
    "task_npoints = complete_window.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the spikes that fall inside the time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perispikes_WT = {}\n",
    "#npys_dir = '/home/maspe/filer/SERT/ALL/npys/'\n",
    "\n",
    "\n",
    "for mouse in all_mice_WT.keys():\n",
    "    clock = time.time()\n",
    "\n",
    "    all_spikes = all_units_WT[mouse]\n",
    "    peristimulus_spikes = np.zeros((len(all_spikes), task_npoints))   \n",
    "    \n",
    "    print('Processing mouse %s...' % mouse)\n",
    "    \n",
    "    for unit in range(len(all_spikes)):\n",
    "        peristimulus_spikes[unit, :] = np.isin(all_epochs_WT[mouse], all_spikes[unit])#, axes=(1, 0))\n",
    "#         peristimulus_spikes[:, :, ] = np.transpose(np.isin(epochs_matrix, all_spikes[unit]), axes=(0, 2, 1))\n",
    "# #         peristimulus_spikes = np.transpose(np.array(peristimulus_spikes), axes=(0, 2, 1))\n",
    "        \n",
    "    all_perispikes_WT[mouse] = peristimulus_spikes\n",
    "    #np.save(npys_dir + mouse + '_perispikes_WT.npy', peristimulus_spikes)\n",
    "    \n",
    "print('Done in {} s.!'.format(time.time() - clock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perispikes_WT['SERT1597'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perispikes_KO = {}\n",
    "\n",
    "for mouse in all_mice_KO.keys():\n",
    "    all_spikes = all_units_KO[mouse] \n",
    "    peristimulus_spikes = np.zeros((len(all_spikes), task_npoints))   \n",
    "    \n",
    "    print('Processing mouse %s...' % mouse)\n",
    "\n",
    "    for unit in range(len(all_spikes)):\n",
    "        peristimulus_spikes[unit, :] = np.isin(all_epochs_KO[mouse], all_spikes[unit])\n",
    "        \n",
    "    #np.save(npys_dir + mouse + '_perispikes_KO.npy', peristimulus_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perispikes_KO.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
